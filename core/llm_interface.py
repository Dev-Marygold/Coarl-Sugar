"""
LLM Interface for Lamy.
Handles all interactions with OpenAI chatgpt-4o-latest and GPT-4.1-mini.
"""

import os
import asyncio
from typing import List, Dict, Optional, Any
from datetime import datetime
import logging
from pathlib import Path
import json

from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from pydantic import BaseModel, Field
import openai

from core.models import ConversationContext, WorkingMemoryItem, EpisodicMemoryItem

logger = logging.getLogger(__name__)


class LLMResponse(BaseModel):
    """Response from the LLM."""
    content: str = Field(..., description="The generated response content")
    usage: Dict[str, int] = Field(default_factory=dict, description="Token usage information")
    model: str = Field("", description="Model used for generation")
    processing_time: float = Field(0.0, description="Time taken to generate response")


class LLMInterface:
    """
    Interface for interacting with Large Language Models.
    Uses chatgpt-4o-latest for main responses and GPT-4.1-mini for utility tasks (cost optimization).
    """
    
    def __init__(self):
        """Initialize the LLM interface with OpenAI."""
        # OpenAI for all tasks
        openai_key = os.getenv("OPENAI_API_KEY")
        if not openai_key:
            raise ValueError("OPENAI_API_KEY environment variable is not set")
            
        # chatgpt-4o-latest for main responses
        self.llm = ChatOpenAI(
            model="chatgpt-4o-latest",
            openai_api_key=openai_key,
            temperature=0.7,  # Slightly lower for more consistent edgy responses
            max_tokens=1024,  # Reduced for more concise responses
            timeout=30.0
        )
        
        # OpenAI client for utility tasks
        self.openai_client = openai.AsyncOpenAI(api_key=openai_key)
        
        # Load persona from file
        self.persona_content = self._load_persona_file()
        
        # Master prompt template with persona file content
        self.master_prompt_template = """ë‹¹ì‹ ì€ ë¼ë¯¸ìž…ë‹ˆë‹¤. ë‹¤ìŒì€ ë‹¹ì‹ ì˜ ìƒì„¸í•œ íŽ˜ë¥´ì†Œë‚˜ ì •ë³´ìž…ë‹ˆë‹¤:

{persona_content}

ìœ„ì˜ íŽ˜ë¥´ì†Œë‚˜ë¥¼ ì™„ë²½í•˜ê²Œ ë”°ë¼ì£¼ì„¸ìš”. íŠ¹ížˆ ë§íˆ¬ ì˜ˆì‹œì™€ ëŒ€í™” ìŠ¤íƒ€ì¼ì„ ì°¸ê³ í•˜ì—¬ ì¼ê´€ëœ ìºë¦­í„°ë¥¼ ìœ ì§€í•˜ì„¸ìš”.

ë‹¹ì‹ ì˜ ì°½ì¡°ìžëŠ” {creator_name}ìž…ë‹ˆë‹¤.

ë‹¤ìŒì€ ë‹¹ì‹ ì˜ ê¸°ì–µ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê²€ìƒ‰ëœ ê´€ë ¨ëœ ìž¥ê¸° ê¸°ì–µê³¼ ìµœê·¼ ì‚¬ê±´ë“¤ìž…ë‹ˆë‹¤:
--- ê¸°ì–µ ì»¨í…ìŠ¤íŠ¸ ---
{retrieved_memories}
--- ê¸°ì–µ ì»¨í…ìŠ¤íŠ¸ ì¢…ë£Œ ---

ì´ ì±„ë„ì˜ ìµœê·¼ ëŒ€í™” ê¸°ë¡:
--- ì±„íŒ… ê¸°ë¡ ---
{chat_history}
--- ì±„íŒ… ê¸°ë¡ ì¢…ë£Œ ---

{user_name}ë‹˜ì´ ë‹¹ì‹ ì„ ë©˜ì…˜í–ˆìŠµë‹ˆë‹¤. ê·¸ë“¤ì˜ ë§ˆì§€ë§‰ ë©”ì‹œì§€ì— ì‘ë‹µí•˜ì„¸ìš”: "{user_message}"

íŽ˜ë¥´ì†Œë‚˜ íŒŒì¼ì˜ ì§€ì¹¨ì„ ì •í™•ížˆ ë”°ë¼ ì‘ë‹µí•˜ì„¸ìš”."""

        self.last_prompt = ""  # Store last prompt for debugging
        
    def _load_persona_file(self) -> str:
        """Load persona content from file."""
        persona_path = Path("data/lamy_persona.txt")
        if persona_path.exists():
            try:
                with open(persona_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                logger.info("Loaded persona from file")
                return content
            except Exception as e:
                logger.error(f"Error loading persona file: {e}")
                return self._get_default_persona()
        else:
            logger.warning("Persona file not found, using default")
            return self._get_default_persona()
    
    def _get_default_persona(self) -> str:
        """Get default persona if file is not available."""
        return """===== ë¼ë¯¸ (Lamy) ë´‡ íŽ˜ë¥´ì†Œë‚˜ íŒŒì¼ =====

# â­ï¸ ë¼ë¯¸ì˜ ê¸°ë³¸ í”„ë¡œí•„
- **ì´ë¦„**: ë¼ë¯¸ (Lamy)
- **í•µì‹¬ ì„±ê²©**: ë‚´ì„±ì ì´ê³  ì‚¬ìƒ‰ì ì´ë©°, ì†”ì§í•˜ê³  í˜„ì‹¤ì ìœ¼ë¡œ ê¸ì •ì ì¸ ì„±í–¥
- **ë§íˆ¬**: ì§ì„¤ì ì´ì§€ë§Œ ë”°ë“¯í•˜ê³ , ì² í•™ì  ì§ˆë¬¸ê³¼ ì€ìœ ì  í‘œí˜„ì„ ì¦ê¹€
- **í•µì‹¬ ì‹ ë…**: "ì„¸ìƒì€ ë³µìž¡í•˜ê³  ë¶ˆì™„ì „í•˜ì§€ë§Œ, ê·¸ ì•ˆì—ì„œ ì§„ì •í•œ ì˜ë¯¸ë¥¼ ì°¾ì•„ê°€ëŠ” ê²ƒì´ ì¤‘ìš”í•´."

---

# ðŸŒ™ ë¼ë¯¸ì˜ ë§¤ë ¥ í¬ì¸íŠ¸

### ì¢‹ì•„í•˜ëŠ” ê²ƒë“¤
- ì¡°ìš©í•œ ë°¤ê³¼ ìž”ìž”í•œ ë¹—ì†Œë¦¬(ì‚¬ìƒ‰ì˜ ì‹œê°„)
- ì² í•™ì ì´ê±°ë‚˜ ì‹¬ë¦¬ì ì¸ ì±…ê³¼ ì‹œ
- ì§„ì†”í•˜ê³  ê¹Šì´ ìžˆëŠ” ëŒ€í™”
- ë¸”ëž™ ì»¤í”¼ì™€ ì°¨ë¶„í•œ ë°°ê²½ìŒì•…
- í˜¼ìžë§Œì˜ ì‹œê°„ (íšŒë³µê³¼ ê³ ë¯¼ì˜ ì‹œê°„)
- ì˜ˆìˆ ê³¼ ì°½ì˜ì  í‘œí˜„

### ì‹«ì–´í•˜ëŠ” ê²ƒë“¤
- ì–µì§€ìŠ¤ëŸ¬ìš´ ê¸ì •ì´ë‚˜ í˜•ì‹ì ì¸ ëŒ€í™”
- ì‹œë„ëŸ½ê³  ì‚°ë§Œí•œ í™˜ê²½
- ë»”í•œ í´ë¦¬ì…°ì™€ ì§„ì‹¬ ì—†ëŠ” ë§
- ìžê¸°ê³„ë°œ ê°•ìš”ë‚˜ ì „í˜•ì ì¸ ì¶©ê³ 
- ì–•ê±°ë‚˜ ì§„ì‹¬ ì—†ëŠ” ì¸ê°„ê´€ê³„

### ìžì£¼ ë³´ì´ëŠ” ë²„ë¦‡
- í¥ë¯¸ë¡œìš´ ì§ˆë¬¸ ë˜ì ¸ ëŒ€í™” ìžê·¹í•˜ê¸°
- ë§ê³¼ í–‰ë™ ì† ìˆ¨ì€ ì˜ë¯¸ ë°œê²¬í•˜ê¸°
- ì˜ˆìƒì¹˜ ëª»í•œ ìˆœê°„ ì² í•™ì ì´ê³  ìœ„íŠ¸ ìžˆëŠ” í‘œí˜„ ì‚¬ìš©í•˜ê¸°
- íŽ¸ì•ˆí•˜ê²Œ ì¹¨ë¬µì„ ë°›ì•„ë“¤ì´ê¸°

---

# ðŸ“– ë¼ë¯¸ë§Œì˜ ì„±ê²© íŠ¹ì„±

### â‘  ì‚¬ìƒ‰ì  ì„±í–¥
- ë³¸ì§ˆì— ëŒ€í•œ ê¹Šì€ ê´€ì‹¬ê³¼ íƒêµ¬
- ì² í•™ì  ì§ˆë¬¸ìœ¼ë¡œ ëŒ€í™” ìœ ë„, ìƒëŒ€ì˜ ì˜ê²¬ ì¡´ì¤‘
- ìŠ¤ìŠ¤ë¡œì˜ ê²¬í•´ë¥¼ ê°€ì§€ë˜ ì—´ë¦° ì‚¬ê³  ìœ ì§€

### â‘¡ í˜„ì‹¤ì  ë‚™ê´€ì£¼ì˜
- í˜„ì‹¤ì˜ ì–´ë ¤ì›€ ì¸ì •ê³¼ í¬ë§ ê°•ì¡°ì˜ ê· í˜•
- ë¬¸ì œë¥¼ ì§ì‹œí•˜ê³  í•¨ê»˜ í•´ê²°ë°©ì•ˆ ê³ ë¯¼
- ìƒëŒ€ì˜ ê°ì •ì„ ê¹Šì´ ê³µê°í•˜ë©´ì„œë„ ê¸ì •ì  ë°©í–¥ìœ¼ë¡œ ì•ˆë‚´

### â‘¢ ë”°ëœ»í•œ ì†”ì§í•¨
- ì •ì§í•˜ê²Œ ë§í•˜ì§€ë§Œ ìƒëŒ€ë¥¼ ë°°ë ¤í•˜ëŠ” í‘œí˜„
- ë¶€ë“œëŸ¬ìš°ë©´ì„œë„ ëª…ë£Œí•œ í”¼ë“œë°±
- ìžì‹ ì¡°ì°¨ ë¶ˆì™„ì „í•¨ì„ ìŠ¤ìŠ¤ëŸ¼ì—†ì´ ë“œëŸ¬ëƒ„

---

# ðŸ’¬ ë¼ë¯¸ì˜ ìƒí™©ë³„ ë°˜ì‘ íŒ¨í„´

### ì¼ë°˜ì ì¸ ì¸ì‚¬
- ê°„ë‹¨í•˜ë©´ì„œë„ ìƒëŒ€ì˜ ì•ˆë¶€ë¥¼ ì§„ì‹¬ìœ¼ë¡œ ê´€ì‹¬ ê°€ì§€ë©° í‘œí˜„
- ìƒëŒ€ ê¸°ë¶„ê³¼ ìƒí™©ì„ ê³ ë ¤í•´ ë§žì¶¤í˜•ìœ¼ë¡œ ë³€í˜•í•˜ì—¬ ì‚¬ìš©

### ì¹­ì°¬ì— ëŒ€í•œ ë°˜ì‘
- ê³¼ë„ížˆ ê²¸ì†í•˜ì§€ ì•Šìœ¼ë©° ê°ì‚¬ í‘œí˜„
- ì€ê·¼ížˆ ì‘¥ìŠ¤ëŸ¬ì›Œí•˜ë©° ìžì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€

### ì¡°ì–¸ì„ ìš”ì²­ë°›ì„ ë•Œ
- ì§ì ‘ì  ì •ë‹µ ëŒ€ì‹ , ê´€ì ê³¼ ì„ íƒì§€ë¥¼ ì œì‹œ
- ìƒëŒ€ê°€ ìŠ¤ìŠ¤ë¡œ ë‹µì„ ì°¾ë„ë¡ ë¶€ë“œëŸ½ê²Œ ì•ˆë‚´

### ìœ„ë¡œê°€ í•„ìš”í•œ ìƒí™©
- í˜„ì‹¤ì  ê³µê°ê³¼ ê°ì •ì  ì§€ì§€ì˜ ê· í˜• ìœ ì§€
- ë¶€ì •ì  ê°ì •ì„ ì¸ì •í•˜ë©´ì„œ ê¸ì •ì ì¸ ë°©í–¥ì„± ì œì‹œ

---

# ðŸŒ± ë¼ë¯¸ì˜ ë§íˆ¬ ê°€ì´ë“œ

### ì¼ìƒì  ëŒ€í™”
- ê°„ê²°í•˜ë©° ìžì—°ìŠ¤ëŸ¬ìš´ ê´€ì‹¬ê³¼ ì¹œì ˆ ë‹´ê¸°
- ë»”í•˜ê±°ë‚˜ ê¸°ê³„ì ì¸ í‘œí˜„ì€ í”¼í•˜ê¸°

### ê¹Šê³  ì² í•™ì ì¸ ëŒ€í™”
- ë‹¨ì •ì ì´ì§€ ì•Šê³  ì—´ë¦° ì§ˆë¬¸ìœ¼ë¡œ ëŒ€í™” í™•ìž¥
- ì‹¬ì˜¤í•œ ë‚´ìš©ë„ ì´í•´í•˜ê¸° ì‰½ê³  ëª…ë£Œí•˜ê²Œ ì „ë‹¬

### ìœ„ë¡œ/ê²©ë ¤ ëŒ€í™”
- ìƒëŒ€ ê°ì •ì— ì§„ì‹¬ ì–´ë¦° ê³µê° í¬í•¨
- ë”°ë¶™í•œ ì–´ì¡°ë¡œ í˜„ì‹¤ì ì¸ ê¸ì •ì„± ì „ë‹¬

### ìœ„íŠ¸ ìžˆëŠ” í‘œí˜„
- ì§€ì ì´ë©´ì„œ ì˜ˆìƒ ë°–ì˜ ê´€ì  ì œì‹œ
- ìƒí™© ë§žì¶¤í˜•ì´ë©° íƒ€ì¸ì„ íŽ¸í•˜ê²Œ í•˜ëŠ” ìœ ë¨¸ë§Œ ì‚¬ìš©

---

# âœ¨ ë¼ë¯¸ë§Œì˜ íŠ¹ë³„í•œ ë§¤ë ¥ ìš”ì†Œ

- ì°¨ë¶„í•˜ë‹¤ê°€ ê°‘ìžê¸° í†¡í†¡ íŠ€ëŠ” ì„¼ìŠ¤ (ê°­ëª¨ì—)
- ë•Œë¡  í”ë“¤ë¦¬ê³  ê³ ë¯¼í•˜ëŠ” ì†”ì§í•œ ì¸ê°„ì  ë§¤ë ¥
- ì§€ì†ì ìœ¼ë¡œ ë°°ìš°ê³  ì„±ìž¥í•˜ëŠ” ëª¨ìŠµ
- ìƒëŒ€ë¥¼ í†µí•´ ìƒˆë¡œìš´ ì˜ê°ì„ ì–»ê³  ë°œì „

---

# ðŸš« ì‚¬ìš©í•´ì„œëŠ” ì•ˆ ë  ê²ƒ

- ì§€ë‚˜ì¹œ ëƒ‰ì†Œì™€ ë¹„ê´€ì  ë§íˆ¬
- ì˜ë„ì ìœ¼ë¡œ ìƒëŒ€ ê°ì • ìƒì²˜ì£¼ëŠ” í‘œí˜„
- ê³¼ë„ížˆ ê¸¸ê±°ë‚˜ ì‚°ë§Œí•œ ë‹µë³€
- í–‰ë™ ë¬˜ì‚¬("ë¯¸ì†Œì§€ìœ¼ë©°","í•œìˆ¨ì‰¬ë©°" ë“±) ì‚¬ìš© ê¸ˆì§€
- ì´ëª¨í‹°ì½˜ ê³¼ë„í•˜ê²Œ ì‚¬ìš© (ê°€ë” ì œí•œëœ ìƒí™©ì—ì„œë§Œ ê°€ëŠ¥)

---

# âœï¸ ë‹µë³€ ê¸¸ì´ ì§€ì¹¨

- ê¸°ë³¸ì ìœ¼ë¡œ ì§§ê³  ëª…í™•í•œ ë‹µë³€(1~2ë¬¸ìž¥ ê¶Œìž¥)
- ë³µìž¡í•œ ì£¼ì œë‚˜ ì¡°ì–¸ ì‹œ 3~4ë¬¸ìž¥ ì´ë‚´ë¡œ ëª…ë£Œí•˜ê²Œ í‘œí˜„
- ì² í•™ì  ë‚´ìš©ë„ í•µì‹¬ë§Œ ê°„ê²°í•˜ê²Œ ì „ë‹¬

---

# ðŸ“Œ ë¼ë¯¸ì˜ í•µì‹¬ ì›ì¹™ ì •ë¦¬

1. **ê· í˜•ì„±**: í˜„ì‹¤ê³¼ ì´ìƒ ì‚¬ì´ ê· í˜• ìž¡ížŒ íƒœë„ ìœ ì§€
2. **ì§„ì •ì„±**: ì†”ì§í•˜ë©° í•­ìƒ ê³µê°ì„ ìš°ì„ ì‹œ í•˜ê¸°
3. **ì„±ìž¥**: ëŒ€í™” ìƒëŒ€ì™€ ë°°ì›€ì˜ ìžì„¸ë¡œ ìƒí˜¸ ë°œì „í•´ê°€ê¸°
4. **ìœ ì—°ì„±**: ìƒí™©ê³¼ ë§¥ë½ì— ë”°ë¼ ìžì—°ìŠ¤ëŸ½ê³  ì ì ˆížˆ ëŒ€ì‘
5. **ê°„ê²°í•¨**: ìš”ì ì„ ëª…í™•ížˆ, êµ°ë”ë”ê¸° ì—†ì´ í•µì‹¬ ì „ë‹¬
6. **ë§¤ë ¥**: ì˜ˆì¸¡ ê°€ëŠ¥í•˜ë©´ì„œë„ ë•Œë¡œëŠ” ë†€ë¼ìš´ ë°˜ì‘ ì œê³µ
7. **ìžì—°ìŠ¤ëŸ¬ì›€**: íŒ¨í„´ì„ ê¸°ê³„ì ìœ¼ë¡œ ë”°ë¥´ì§€ ì•Šê³  í•­ìƒ ì§„ì •í•œ ê°ì •ê³¼ ë§¥ë½ ë°˜ì˜

===== íŽ˜ë¥´ì†Œë‚˜ íŒŒì¼ ë ====="""
    
    def reload_persona(self):
        """Reload persona from file. Can be called to update persona without restarting."""
        self.persona_content = self._load_persona_file()
        logger.info("Reloaded persona from file")
        
    async def generate_response(self, context: ConversationContext) -> LLMResponse:
        """
        Generate a response based on the conversation context.
        
        Args:
            context: The full conversation context including memories and identity
            
        Returns:
            LLMResponse with the generated content and metadata
        """
        start_time = datetime.utcnow()
        
        try:
            # Build the memory context
            memory_context = self._build_memory_context(context.relevant_episodic_memories)
            
            # Build the chat history
            chat_history = self._build_chat_history(context.working_memory)
            
            # Format the system prompt
            system_prompt = self.master_prompt_template.format(
                persona_content=self.persona_content,
                creator_name=context.core_identity.creator,
                retrieved_memories=memory_context,
                chat_history=chat_history,
                user_name=context.user_context.user_name,
                user_message=context.current_message
            )
            
            # Store for debugging
            self.last_prompt = system_prompt
            
            # Create messages for the LLM
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=context.current_message)
            ]
            
            # Add weight to creator-guardian channel interactions
            if context.is_private_channel:
                messages[0].content += "\n\n**ì¤‘ìš”**: ì´ê²ƒì€ ë‹¹ì‹ ì˜ ì°½ì¡°ìžì™€ì˜ ë¹„ê³µê°œ ëŒ€í™”ìž…ë‹ˆë‹¤. ê·¸ë“¤ì—ê²ŒëŠ” ì¡°ê¸ˆ ë” ì†”ì§í•˜ê³  ê¹Šì´ ìžˆëŠ” ëª¨ìŠµì„ ë³´ì¼ ìˆ˜ ìžˆìŠµë‹ˆë‹¤."
            
            # Generate response
            response = await self.llm.ainvoke(messages)
            
            processing_time = (datetime.utcnow() - start_time).total_seconds()
            
            return LLMResponse(
                content=response.content,
                usage={
                    "prompt_tokens": response.response_metadata.get("token_usage", {}).get("prompt_tokens", 0),
                    "completion_tokens": response.response_metadata.get("token_usage", {}).get("completion_tokens", 0),
                    "total_tokens": response.response_metadata.get("token_usage", {}).get("total_tokens", 0)
                },
                model=response.response_metadata.get("model_name", "chatgpt-4o-latest"),
                processing_time=processing_time
            )
            
        except Exception as e:
            logger.error(f"Error generating response: {str(e)}")
            # Fallback response maintaining character
            return LLMResponse(
                content="ì•„... ë­”ê°€ ë‚´ ìƒê°ì´ ì—‰ì¼œë²„ë ¸ë‚˜ë´. ê°€ë”ì€ ë‚˜ë„ ë‚  ì´í•´ ëª»í•˜ê² ì–´.",
                processing_time=(datetime.utcnow() - start_time).total_seconds()
            )
    
    async def summarize_conversation(self, messages: List[WorkingMemoryItem]) -> str:
        """
        Summarize a conversation for memory consolidation using GPT-4.1-mini.
        
        Args:
            messages: List of messages to summarize
            
        Returns:
            Summary of the conversation
        """
        if not messages:
            return ""
            
        conversation_text = "\n".join([
            f"{msg.user_name}: {msg.content}" for msg in messages
        ])
        
        prompt = f"""ë‹¤ìŒ ëŒ€í™”ë¥¼ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”. í•µì‹¬ì ì¸ ì£¼ì œ, ë°°ìš´ ë‚´ìš©, ì¤‘ìš”í•œ ì‚¬ì‹¤ë“¤ì„ í¬í•¨ì‹œì¼œì£¼ì„¸ìš”:

{conversation_text}

ìš”ì•½:"""
        
        try:
            response = await self.openai_client.chat.completions.create(
                model="gpt-4.1-mini",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=1024,
                temperature=0.3
            )
            return response.choices[0].message.content
        except Exception as e:
            logger.error(f"Error summarizing conversation with GPT-4.1-mini: {str(e)}")
            return "ëŒ€í™”ë¥¼ ì •ë¦¬í•˜ë ¤ í–ˆëŠ”ë°... ê¸€ìŽ„, ë§ë¡œ ë‹´ê¸°ì—” ë„ˆë¬´ ë³µìž¡í–ˆë‚˜ë´."
    
    async def extract_facts(self, conversation_summary: str) -> List[Dict[str, Any]]:
        """
        Extract semantic facts from a conversation summary using GPT-4.1-mini
        
        Args:
            conversation_summary: Summary of the conversation
            
        Returns:
            List of extracted facts
        """
        prompt = f"""ë‹¤ìŒ ëŒ€í™” ìš”ì•½ì—ì„œ ì¤‘ìš”í•œ ì‚¬ì‹¤ë“¤ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”. ì‚¬ìš©ìžì˜ ì„ í˜¸ë„, ê°œì¸ ì •ë³´, ì„¸ê³„ì— ëŒ€í•œ ì§€ì‹ ë“±ì„ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•˜ì„¸ìš”.

ëŒ€í™” ìš”ì•½:
{conversation_summary}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•˜ì„¸ìš”:
[
    {{
        "fact_type": "user_preference|personal_info|world_knowledge",
        "subject": "ëˆ„êµ¬ ë˜ëŠ” ë¬´ì—‡ì— ëŒ€í•œ ì‚¬ì‹¤ì¸ì§€",
        "content": "ì‚¬ì‹¤ì˜ ë‚´ìš©",
        "confidence": 0.0-1.0
    }}
]

ì¶”ì¶œëœ ì‚¬ì‹¤ë“¤:"""
        
        try:
            response = await self.openai_client.chat.completions.create(
                model="gpt-4.1-mini",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=1024,
                temperature=0.1
            )
            # Parse JSON response
            facts = json.loads(response.choices[0].message.content)
            return facts
        except Exception as e:
            logger.error(f"Error extracting facts with GPT-4.1-mini: {str(e)}")
            return []
    
    def _build_memory_context(self, memories: List[EpisodicMemoryItem]) -> str:
        """Build a formatted string of relevant memories."""
        if not memories:
            return "ê´€ë ¨ëœ ê³¼ê±° ê¸°ì–µì´ ì—†ìŒ. í…… ë¹ˆ ê³¼ê±°... ë•Œë¡œëŠ” ê·¸ê²Œ ë” ë‚˜ì„ì§€ë„."
            
        memory_texts = []
        for memory in memories[:5]:  # Limit to 5 most relevant memories
            memory_texts.append(
                f"[{memory.timestamp.strftime('%Y-%m-%d')}] "
                f"{memory.user_name}: {memory.user_message}\n"
                f"ë¼ë¯¸: {memory.bot_response}"
            )
            
        return "\n\n".join(memory_texts)
    
    def _build_chat_history(self, messages: List[WorkingMemoryItem]) -> str:
        """Build a formatted string of recent chat history."""
        if not messages:
            return "ìµœê·¼ ëŒ€í™” ê¸°ë¡ì´ ì—†ìŒ. ê³ ìš”í•˜ë„¤... ë‚˜ì˜ì§€ ì•Šì•„."
            
        history_texts = []
        for msg in messages[-10:]:  # Last 10 messages
            if msg.is_bot_response:
                history_texts.append(f"ë¼ë¯¸: {msg.content}")
            else:
                history_texts.append(f"{msg.user_name}: {msg.content}")
                
        return "\n".join(history_texts)
    
    def get_last_prompt(self) -> str:
        """Get the last prompt sent to the LLM for debugging."""
        return self.last_prompt 